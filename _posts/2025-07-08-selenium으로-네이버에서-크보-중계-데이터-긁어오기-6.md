---
layout: post
title: Selenium으로 네이버에서 크보 중계 데이터 긁어오기 6
category: KBO, 중계긁기
tags:
- KBO
- 스크래핑
- python
date: 2025-07-08 21:44 +0900
---
이제 일자를 이동하면서 각 일자마다 경기 중계 주소를 얻어 이전에 만든 코드를 통해 데이터를 긁어오게 만들어보자.

먼저 이 과정에서 어떤 식으로 코드가 동작해야 할 지를 생각해보자. 우선 경기가 있는 일자의 페이지로 이동하고, 각 경기의 중계 페이지를 저장해둔다. 그리고 이 중계 페이지들을 이동하면서 데이터들을 긁고, 경기가 있는 다음 일자로 이동하여 반복하면 된다. 그리고 일정 페이지의 구조를 보자면, 원하는 일자로 바로 이동할 수 있다. 즉, 달을 이동한 뒤에, 활성화가 되어 있는(KBO 경기가 있는지는 알 수 없지만) 일자들을 모두 저장해둔 뒤, 각 일자별 일정 페이지로 이동하면 될 것이다. 그리고 달의 마지막 일자에 이른 뒤에는 마지막 일 페이지로 돌아와 다음 달로 이동하는 버튼을 눌러 다음 달로 이동하고 반복... 이러한 구조로 짜면 잘 작동할 것 같다.

그러면 우선 특정 달에서 활성화가 되어있는 날짜들을 모아서 반환하는 코드를 짜보...기 전에 편의를 위해 CSS_SELECTOR로 요소 찾는 코드를 래핑해서 단순화 시키자. 대충 다음과 같이 짤 수 있다.

```python
def find_element_by_CSSS(self, parent, query):
    return parent.find_element(By.CSS_SELECTOR, query)
```

이제 날짜 버튼에 해당하는 html 코드들을 모아 활성화 된 것들만 남겨놓은 뒤, 해당하는 일자들을 반환하는 코드를 짜보자.
```python
def get_activated_date(self):
    main_section = self.find_element_CSSS(self.driver, 'div[class^="Home_container"]')
    date_area = self.find_element_CSSS(main_section, 'div[class^="CalendarDate_schedule_date_area"]')
    date_tab = self.find_element_CSSS(date_area, 'div[class^=CalendarDate_calendar_tab_wrap]')
    date_buttons = date_tab.find_elements(By.CSS_SELECTOR, 'button')

    activated_dates = []
    for btn in date_buttons:
        if btn.is_enabled():
            btn_date = self.find_element_CSSS(btn, 'em')
            activated_dates.append(int(btn_date.get_attribute('innerHTML')))

    return activated_dates

```
일자 버튼을 모두 찾은 뒤 하나하나 검사하면서 활성화되어있으면 내부의 일자를 표시하는 텍스트들을 activated_dates에 넣어두고 반환하는 구조이다. btn_date.text를 이용해 가져오면 화면에서 벗어난 버튼의 일자를 가져오지 못하길래 innerHTML을 직접 가져오는 방식으로 구현했다. int로 바꾸는 이유는 나중에 날짜 형식을 맞출 때 편하게 하기 위해서이다.

테스트 코드를 다음과 같이 짜서 작동시켜보자.
```python
if __name__ == "__main__":
    scrapper = Scrapper()
    scrapper.driver.get("https://m.sports.naver.com/kbaseball/schedule/index")
    time.sleep(2)

    print(scrapper.get_activated_date())

    os.system("pause")
```
작성일 기준 7월의 활성화된 일자들을 성공적으로 출력하는 것을 볼 수 있다.

이제 특정 일자의 일정 페이지로 이동해서 해당 일자의 경기 주소 페이지를 얻어오는 코드를 작성해보자. 연월일을 가지고 일정 주소를 작성한 뒤, 그 페이지로 이동해서 종료된 경기의 주소를 얻는 방식이다.

```python
# 일자 지정하여 일정 주소 반환
def get_schedule_page_url(self, year, month, date):
    base = "https://m.sports.naver.com/kbaseball/schedule/index?date="
    date = datetime.date(year, month, date).strftime("%Y-%m-%d")
    
    return base + date

# 특정 일자에서 경기 페이지 주소 얻기
def get_game_urls(self, year, month, date):
    self.driver.get(self.get_schedule_page_url(year,month,date))
    time.sleep(1)

    main_section = self.find_element_CSSS(self.driver, 'div[class^="Home_container"]')
    match_group = main_section.find_elements(By.CSS_SELECTOR, 'div[class^="ScheduleAllType_match_list_group"]')

    for grp in match_group:
        a = self.find_element_CSSS(grp, 'div[class^="ScheduleAllType_title_area"]')
        em = self.find_element_CSSS(a, 'em')
        if em.text == "KBO리그":
            target_group = grp
            break
        else:
            target_group = None
    
    if target_group is None:
        return -1

    match_urls = []    
    matches = target_group.find_elements(By.CSS_SELECTOR, 'li[class^="MatchBox_match_item"]')
    for match in matches:
        match_status = self.find_element_CSSS(match, 'em[class^=MatchBox_status]')
        if match_status.text == "종료":
            match_urls.append(self.find_element_CSSS(match, 'a[class^="MatchBox_link"]').get_attribute('href'))

    return match_urls
```
우선은 일자를 지정하면 해당 일자의 일정 페이지를 만드는 함수를 만들고, 이후 해당 페이지로 이동하여 match group 중 KBO리그 그룹을 찾아 group 안의 경기 상태가 종료인 경기의 주소들을 긁어오는 구조이다.

확인을 위해 우천취소 경기가 포함된 2025년 6월 24일을 대상으로 테스트코드를 다음과 같이 짜서 실행해보면 정상적으로 취소되지 않은 3경기의 url을 출력하는 것을 확인할 수 있다.

```python
if __name__ == "__main__":
    scrapper = Scrapper()
    print(scrapper.get_game_urls(2025, 6, 24))

    os.system("pause")
```
