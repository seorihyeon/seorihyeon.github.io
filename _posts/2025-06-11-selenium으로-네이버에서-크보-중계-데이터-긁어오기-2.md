---
layout: post
title: Selenium으로 네이버에서 크보 중계 데이터 긁어오기 2
category: KBO, 중계긁기
tags: [KBO, 스크래핑, python]
date: 2025-06-11 23:24 +0900
---
네이버에서 무슨 데이터를 긁어오면 될 지를 알아내었으니, 이제 Selenium을 활용하여 자동으로 긁어오는 프로그램을 만들어 보자. 일단은 스크래핑을 수행하는 클래스 scrapper를 만드는 것부터 시작한다.

Selenium은 자바스크립트등으로 동적으로 생성된 정보가 필요할 때 사용하는 라이브러리이다. 사용해보면 알겠지만, 브라우저를 실행시켜 직접 그 페이지를 간 뒤에, 그 브라우저에서 직접 html 소스를 뽑아온다. (특정 옵션을 사용하면 브라우저 창이 뜨지는 않게 할 수 있는데 그래도 아마 GUI로만 안 되고 백그라운드로는 도는 거니까 틀린 말은 아닐거라 생각한다.) 이를 위해 브라우저를 제어하기 위한 드라이버가 필요한데, 여기서는 크롬 드라이버를 사용했다. 크롬 드라이버는 https://chromedriver.chromium.org/downloads에서 자기가 사용하는 크롬 버전에 맞는 드라이버를 다운 받을 수 있다.

그리고 이전 글에서도 말했듯 네트워크 패킷에 접근해야 하기 때문에 selenium-wire를 사용한다. selenium-wire에 대한 내용은 https://pypi.org/project/selenium-wire/ 에서 확인할 수 있다.

Selenium-wire에서 웹드라이버를 불러오고, 이를 이용해 scrapper 클래스를 초기화한다.

```python
from seleniumwire import webdriver

class Scrapper:
    def __init__(self, wait = 10, path = './games/'):
        chrome_options = webdriver.ChromeOptions()
        chrome_options.add_argument("--no-sandbox")
        #chrome_options.add_argument("--headless")
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36')
        self.driver = webdriver.Chrome(options = chrome_options)
        self.driver.implicitly_wait(wait)
        self.path = path
```

webdriver.ChromeOptions는 드라이버에 적용할 옵션을 저장한다. 각 옵션은 add_argument를 이용하여 추가되며 여기서는 샌드박스 기능을 비활성화하는 "--no-sandbox"옵션과 브라우저의 GUI를 비활성화하는 "--headless" 옵션, user-agent 옵션을 사용하고 있다. 샌드박스 기능은 활성화하면 권한 오류가 떠서 비활성화해두었다. --headless 옵션은 최종적으론 적용하는게 괜찮을 것 같아 넣어놓았지만 테스트중에는 주석처리 해두었다. 오류가 떴을 때 GUI로 브라우저가 어떻게 작동하는지 확인하는 게 원인을 찾기 더 쉽기 때문이다. (고수라면 로그만 봐도 될 터이지만 아쉽게도 그 정도로 실력이 있지는 못하다.) driver.implicitly_wait는 웹사이트 로딩을 기다리는 시간을 설정하며, 인자는 타임아웃시간(최대 대기시간)이다. path는 긁어온 경기 데이터가 저장될 위치를 저장해둔다.

여기까지만 하고 일단 테스트를 해서 Selenium이 정상작동하는지를 확인해보자.
```python
import os

if __name__ == "__main__":
    scrapper = Scrapper()
    os.system("pause")
```

`if __name__ == "__main__"` 은 유명한 구문으로, python에서 직접 실행된 스크립트인지 아닌지를 구분하는 구문이다. 예를들어 위 코드를 import를 통해 불러오게 되면 `__name__`이 모듈 이름이 되기 때문에 저 if문 안의 코드는 실행되지 않는다. if문안의 내용은 단순하게 스크래퍼 클래스를 생성한 뒤 잠깐 실행을 멈추는 코드다. 일시정지 코드인 os.system("pause")를 사용하기 위해 os라이브러리가 임포트 되었다. 실행시키면 Selenium이 사용하는 크롬 브라우저가 실행되는 것을 확인할 수 있다.

![크롬브라우저](/assets/img/post_img/Selenium_2/셀레니움_크롬.png)
_Selenium이 제어하는 크롬 브라우저_

이제 중계페이지에서 목표하는 json 파일을 긁어오는 기능을 만들어보자. 우선 중계 페이지는 페이지 내에서 이닝 페이지를 이동할 때 해당 이닝의 json파일을 가져와 중계 페이지를 생성한다. 즉, Selenium은 이 이닝버튼을 차례로 눌러가면서 받아온 json파일을 저장하면 되는 것이다. 그러니 제일 처음 만들어야 할 기능은 Selenium이 이닝 버튼을 찾아서 누를 수 있도록 하는 기능이다.

![이닝버튼](/assets/img/post_img/Selenium_2/이닝_버튼.png)
_중계 페이지 내의 이닝 버튼_

다시 중계페이지에서 f12로 개발자 메뉴를 열고 열심히 소스를 뒤져보면 이닝 버튼의 소스를 찾을 수 있으니 해당 소스가 위치한 트리 내 위치를 Selenium이 찾을 수 있도록 해보자. 트리 구조의 단계를 하나하나 다 밟는건 너무 간 것 같고, 그렇다고 한번에 가려고 하면 또 어디서 문제가 발생할 수도 있으니 적당히 단계들을 건너뛰면 될 것 같다. 나 같은 경우는 다음과 같이 찾도록 했다.

```python
from selenium.webdriver.common.by import By

main_section = self.driver.find_element(By.CSS_SELECTOR, 'div[class^="Home_main_section"]')
game_panel = main_section.find_element(By.CSS_SELECTOR, 'section[class^="Home_game_panel"]')
tab_list = game_panel.find_element(By.CSS_SELECTOR, 'div[class^="SetTab_tab_list"]')
inning_buttons = tab_list.find_elements(By.CSS_SELECTOR, 'button')
```

find_element는 말 그대로 특정 요소를 찾는 함수이다 find_element는 조건을 만족하는 요소중 가장 처음 값을 반환하며, find_elements는 모든 값을 list로 반환한다. 만약 조건을 만족하는 요소가 발견되지 않을 경우, find_element는 NoSuchElementException 예외를 발생시키며, find_elements는 빈 리스트를 반환한다. 인자인 By.CSS_SELECTOR, 'div~'는 찾기를 원하는 요소의 조건으로 By.CSS_SELECTOR는 css 선택자를 이용해 검색한다는 것이다. div[class^="ABC"]는 클래스명이 ABC로 시작하는 <div>요소를 찾겠다는 뜻이다. 중계 페이지의 소스를 보면 클래스 명 뒤에 뭔가 코드같은 것들이 붙어있는데, 해당 코드가 변하는지 아닌지 알 수 없기 때문에 변하지 않을 것 같은 앞부분만을 이용하여 검색하기 위해 이런 방식을 택하였다.

그렇게 차례로 내려가다가 마지막에 button 선택자를 가지는 이닝 버튼들을 찾아내는 것으로 완료된다.

여기서 주의할 점은 네이버 중계 페이지의 이닝 버튼은 기본적으로 9회까지의 버튼은 존재한다는 점이다. 연장전일 경우는 추가 버튼을 생성하지만, 강우 콜드 등의 경우로 인해 9회 이전에 경기가 종료될 경우 해당 이닝은 버튼을 비활성화 처리한다. 즉, 찾아낸 버튼들 중 활성화된 버튼만 따로 뽑아내는 작업이 필요하다. 이 작업을 포함해서 Scrapper 클래스의 find_inning_button 메소드를 작성하면 다음과 같다.

```python
def find_inning_button(self):
    main_section = self.driver.find_element(By.CSS_SELECTOR, 'div[class^="Home_main_section"]')
    game_panel = main_section.find_element(By.CSS_SELECTOR, 'div[class^="Home_game_panel"]')
    tab_list = game_panel.find_element(By.CSS_SELECTOR, 'div[class^="SetTab_tab_list"]')
    inning_buttons = tab_list.find_elements(By.CSS_SELECTOR, 'button')

    inning_buttons[:] = [btn for btn in inning_buttons if btn.is_enabled()]

    return inning_buttons
```

이제 기능이 제대로 동작하는지 테스트 코드를 다음과 같이 짜서 실행해보자.

```python
import time
from selenium.webdriver.common.action_chains import ActionChains

if __name__ == "__main__":
    scrapper = Scrapper()
    scrapper.driver.get("https://m.sports.naver.com/game/20250419NCHH02025/relay")
    inning_buttons = scrapper.find_inning_button()

    for btn in inning_buttons:
        time.sleep(1)
        ActionChains(scrapper.driver).move_to_element(btn).click(btn).perform()

    os.system("pause")
```

driver.get(link)함수는 Selenium 브라우저가 link주소로 이동하도록 하는 함수이다. 여기서 사용된 주소는 강우콜드가 포함된 25시즌 4월 19일 NC-한화전의 중계 페이지다.
ActionChains는 Selenium이 브라우저 내에서 수행할 동작들을 묶어서 보내주는 역할을 한다. 위 코드같은 경우에는 btn으로 이동(move_to_element)한 뒤, btn을 클릭하는 동작을 수행한다. 그 위의 time.sleep(1)은 작동을 눈으로 확인하기 위해 넣은 것으로, ActionChanis에 의한 동작 사이사이에 1초의 딜레이를 준다.

기능이 잘 만들어졌다면 Selenium과 연결된 브라우저가 중계 페이지로 이동하여 1회부터 5회까지 이닝 페이지를 1초간격으로 이동하게 된다. 코드를 작성하고 실행해보면 잘 실행되는 것을 확인할 수 있다.
