---
layout: post
title: Selenium으로 네이버에서 크보 중계 데이터 긁어오기 7
category: KBO, 중계긁기
tags:
- KBO
- 스크래핑
- python
date: 2025-07-22 21:27 +0900
---
다른 일을 좀 하다가 약 2주만에 다시 작업을 재개했다. 따로 기록을 안하던 예전이었다면 어디까지 했었더라하고 한참 코드를 들여다봤을텐데, 이번엔 그냥 블로그에 올려놓은 글을 읽기만 하면 되니 확실히 편하다. 문서화가 이렇게 중요하다.

이제 할 일은 만들어두었던 코드들을 조립해서 특정 시즌의 모든 경기 기록을 긁어오는 코드를 만들면 되는데, 그 전에 먼저 해야할 일이 있다. 긁어오는 json파일을 보면 불필요한 정보들, 중복되는 정복들이 굉장히 많다. 어짜피 이런 부분들은 전처리를 해줘야 하는 만큼, 긁어오는 단계에서 대략적으로나마 전처리를 해서 저장공간 낭비를 좀 줄여보도록 하자.

우선 라인업 데이터에서 가장 원하는 정보는 선발 라인업 및 후보 선수 목록이다. 그 외에 쓸만할 정보는 gameInfo에 있는 경기 시간이나 경기장 정보가 있다. 다만 gameInfo에 있는 정보들은 다르게 사용해볼수 도 있을 것 같으니 일단 다 저장해두고 나중에 다시 살펴보면 좋을 것 같다.

결론적으로 라인업 데이터에서 남겨둘만한 정보는 gameInfo와 home/awayTeamLineUp항목의 pitcherBullpen(불펜 엔트리), fullLineUp(선발 라인업), batterCandidate(후보 야수 엔트리)이다. 이 정보들만 뽑는 전처리 코드를 다음과 같이 짤 수 있다.

```python
def preprocess_lineup_data(self, lineup_data):
    
    preview_data = lineup_data["result"]["previewData"]
    away_lineup = preview_data["awayTeamLineUp"]
    home_lineup = preview_data["homeTeamLineUp"]

    processed_data = dict(game_info = preview_data["gameInfo"],
                            home_starter = home_lineup["fullLineUp"],
                            home_bullpen = home_lineup["pitcherBullpen"],
                            home_candidate = home_lineup["batterCandidate"],
                            away_starter = away_lineup["fullLineUp"],
                            away_bullpen = away_lineup["pitcherBullpen"],
                            away_candidate = away_lineup["batterCandidate"])
    
    return processed_data
```

이제 라인업 데이터를 얻는 get_lineup_data함수의 마지막에 이 전처리 함수를 실행시키면 된다.

다음은 기록 데이터를 전처리 해보자. 기록데이터에서 원하는 정보는 당연히 기록지 데이터다. 중계데이터를 통해 구성할 수 있지 않나 싶지만 그렇기 때문에 이후 중계데이터가 제대로 뽑혔는지 테스트해볼 용도로 사용해 볼 수 있다. 그러니 pitchersBoxscore과 battersBoxscore를 저장해두도록 하자. 코드는 다음과 같이 짜볼 수 있다.

```python
def preprocess_record_data(self, record_data):
    record = record_data["result"]["recordData"]

    processed_data = dict(pitcher = record["pitchersBoxscore"],
                            batter = record["battersBoxscore"])
    
    return processed_data
```

다음은 이닝 중계데이터 차례다. 여기서는 이닝마다 json파일이 있는데, 다른 부분은 다 버리고 textRelays에 있는 데이터만 있으면 된다.

```python
def preprocess_inning_data(self, inning_data):
    processed_data = inning_data["result"]["textRelayData"]["textRelays"]

    return processed_data
```

별개로 건드는 김에 저번에 매직넘버로 구현해 두었던 라인업/중계/기록 탭 버튼 찾기를 좀 더 안정성 있게 바꿔보려고 한다.

우선 이전에 짜뒀던 find_tab_button 함수를 다음과 같이 바꾼다.
```python
def find_tab_button(self):
    main_section = self.find_element_CSSS(self.driver, 'div[class^="Home_main_section"]')
    game_panel = self.find_element_CSSS(main_section, 'section[class^="Home_game_panel"]')
    game_tab = self.find_element_CSSS(game_panel, 'ul[class^="GameTab_tab_list"]')
    tab_buttons = game_tab.find_elements(By.CSS_SELECTOR, 'button')
    
    tab_button_dict = dict()
    
    for btn in tab_buttons:
        text = self.find_element_CSSS(btn, 'span[class^="GameTab_text"]').text
        tab_button_dict[text] = btn

    return tab_button_dict
```

밑부분에서 버튼 내 텍스트를 통해 버튼을 찾을 수 있게 딕셔너리를 만들어 반환하도록 했다. 이렇게 되면 만약 순서가 바뀌더라도 텍스트를 통해 찾을 수 있게 된다. 이에따라 get_game_data 함수도 다음과 같이 바뀌게 된다.

```python
def get_game_data(self, game_url):
    self.driver.get(game_url)
    tab_buttons = self.find_tab_button()
    lineup_data = self.get_lineup_data(tab_buttons["라인업"])
    inning_data = self.get_inning_data(tab_buttons["중계"])
    record_data = self.get_record_data(tab_buttons["기록"])

    return lineup_data, inning_data, record_data
```

이제 테스트 코드를 다음과 같이 짜서 실행해보면, 정상적으로 원하는 데이터들을 저장하는 것을 볼 수 있다.

```python
if __name__ == "__main__":
scrapper = Scrapper()
urls = scrapper.get_game_urls(2025, 6, 24)
ld, ind, rd = scrapper.get_game_data(urls[0])

with open('lineup.json', 'w') as tgtfile:
    json.dump(ld, tgtfile, ensure_ascii= False, indent = 4)
with open('inning.json', 'w') as tgtfile:
    json.dump(ind, tgtfile, ensure_ascii= False, indent = 4)
with open('record.json', 'w') as tgtfile:
    json.dump(rd, tgtfile, ensure_ascii= False, indent = 4)
```